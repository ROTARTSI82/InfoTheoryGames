% !TeX root = main.tex

\input{"preamb.tex"}

\begin{document}
    \title{Theory}
    \author{Atharv Goel, Grant Yang}
    \maketitle

    \section{Battleship}

    We can use the information from ``you sunk my battleship''? Do we have to determine the bitboard layout or the exact arrangement of ships?

    We want to choose the shot the minimizes $H(A | S)$, where $S$ is the state of hits and misses we see after taking the shot (can be modelled as a Bernoulli random variable for hit or miss), and $A$ is the arrangement of the opponents ships. ???? is this correct?

    From the current game state, we can randomly place battleships in arrangements that satisfy what we observe for hits and misses. We want to sample this space roughly uniformly, even though arrangements where two ships are touching are probably less likely in reality due to that not being a good strategy (on the opponent's part). From these positions, we can count up the percent of time a given unexplored square is occupied, and choose the one that is closest to 50\% probability hit/miss to get the maximum amount of information from our shot. (idk) Looking at the previous paragraph, this is optimal if we simplifying it a lot where we consider the number of arrangements after an outcome to be proportional to the probability of the outcome \[\frac{H(A | S = k)}{\sum_i H(A | S = i)} = P(S=k).\] ????? idk if this is right at all. 

    \newpage

    \section{Hangman}

    We consider Hangman as two different subproblems. The first is picking the optimal letter given a current game. The second is updating the game after receiving information from the user. We begin with the latter.
    
    Since the user can lie, we are uncertain about the game state. Hence, let the current game be the random variable $S$ with probability mass function $p(s)$. Each different possible value of $S$ now represents the simplified game state given that the user lied on a specific turn, or not at all. At the beginning of the game, there is only one possible value for $S$: the \textit{truth node}. Note that the user can only lie once. Thus, as the game progresses, $S$ gains more possible values as the \textit{truth node} splits into a new \textit{truth node} and a \textit{lie node} that assumes the user lied on that turn. However, all the \textit{lie nodes} from previous turns can get filtered by the new information as if it was telling the truth. $p(s)$ is defined as a geometric distribution with $p=0.25$. It is a little arbitrary, but it works.

    For picking the optimal letter, we merely choosever whichever gives the most expected information, across all possibilities for $S$. Each word has a relative frequency sourced from the web. Since the specific value of $S$ does not matter, we condense it into a single state, with each word frequency scaled by $p(s)$ where $s$ is the node that the word is in. Identical words in different nodes are combined such that their $p(s)$ scaled frequencies are added. The phrases are flattened as well. Thus, we obtain a single structure with a random variable $W$ for each word, with a probability mass function obtained from the word frequencies. For a given letter, the expected information is found by summing $-p \log(p)$ over every different pattern in which the letter can appear, where $p$ is the sum of the probabilities of the words to which the pattern corresponds. This is looped over every letter, and whichever one yields the highest expected information is guessed.

\end{document}
